<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://blog.g4.cx</id>
    <title>PCDotFan</title>
    <updated>2020-01-26T15:54:59.667Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://blog.g4.cx"/>
    <link rel="self" href="https://blog.g4.cx/atom.xml"/>
    <subtitle>A straight jacket for your mind.</subtitle>
    <logo>https://blog.g4.cx/images/avatar.png</logo>
    <icon>https://blog.g4.cx/favicon.ico</icon>
    <rights>All rights reserved 2020, PCDotFan</rights>
    <entry>
        <title type="html"><![CDATA[强化学习：[DQN] The Pacman]]></title>
        <id>https://blog.g4.cx/post/the-pacman</id>
        <link href="https://blog.g4.cx/post/the-pacman">
        </link>
        <updated>2020-01-26T15:33:26.000Z</updated>
        <content type="html"><![CDATA[<!--more-->
<h2 id="基本参数">基本参数</h2>
<h3 id="envrender">env.render()</h3>
<figure data-type="image" tabindex="1"><img src="https://cdn.mywpku.com/20200126233632.png" alt="" loading="lazy"></figure>
<p>210x160 的界面，RGB 三通道</p>
<h3 id="action_space-observation_space">action_space / observation_space</h3>
<ul>
<li>action_space：<code>['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT']</code></li>
<li>observation_space：<code>Box(210, 160, 3)</code></li>
</ul>
<h2 id="图像预处理">图像预处理</h2>
<figure data-type="image" tabindex="2"><img src="https://cdn.mywpku.com/20200126233917.png" alt="" loading="lazy"></figure>
<pre><code class="language-python"># 吃豆人颜色
color = np.array([210, 164, 74]).mean()

def preprocess_observation(obs):

    # 裁剪和转换图片尺寸
    img = obs[1:176:2, ::2]
    # 图像转换成灰度
    img = img.mean(axis=2)
    # 加强图像对比
    img[img==color] = 0
    # 正则化，使值分布在 [-1, 1]
    img = (img - 128) / 128 - 1
   return img.reshape(88,80,1)
</code></pre>
<pre><code>在实际的训练过程中颜色属于无关信息，过大的尺寸也会影响训练的速度。因此对图像转换成 88x80 并灰度处理。
</code></pre>
<h2 id="设置网络">设置网络</h2>
<figure data-type="image" tabindex="3"><img src="https://cdn.mywpku.com/20200126233935.png" alt="" loading="lazy"></figure>
<p>因为 DQN 是双网络结构，所以封装一个 <code>q_network</code> 函数便于调用。</p>
<pre><code class="language-python">def q_network(X, name_scope): 
    
    # 初始化
    initializer = tf.contrib.layers.variance_scaling_initializer()

    with tf.variable_scope(name_scope) as scope:
        
        # 初始化卷积层
        layer_1 = conv2d(X, num_outputs=32, kernel_size=(8,8), stride=4, padding='SAME', weights_initializer=initializer)

	   # Tensorboard 绘图用
        tf.summary.histogram('layer_1',layer_1)

        layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(4,4), stride=2, padding='SAME', weights_initializer=initializer)
        tf.summary.histogram('layer_2',layer_2)

        layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding='SAME', weights_initializer=initializer)
        tf.summary.histogram('layer_3',layer_3) 

        # 在放入全连接层前先展平第三层结果
        flat = flatten(layer_3)
        fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)           

        tf.summary.histogram('fc',fc)
        output = fully_connected(fc, num_outputs=9, activation_fn=None, weights_initializer=initializer)
        tf.summary.histogram(‘output’,output) # 存储网络参数（权重等）

        vars = {
        v.name[len(scope.name):]: v for v in tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)
        }

        return vars, output
</code></pre>
<h2 id="epsilon-greedy-算法">Epsilon-Greedy 算法</h2>
<p>由于初始阶段「经验」不足，因此固定的行为模式未必要比随机的探索环境更好（实际上在前期，随机动作大多都要比使用经验执行的动作效果要好）。因此我们可以设定一个 Epislon 值来表示执行随机操作的概率，智能体在选择动作时有两种选项：一是执行随机操作（概率为 Epsilon），二是执行根据经验得出的最佳动作（概率为 1 - Epsilon）。</p>
<pre><code class="language-python">epsilon = 0.5 
eps_min = 0.05
eps_max = 1.0
eps_decay_steps = 500000

def epsilon_greedy(action, step):
    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)
    if np.random.rand() &lt; epsilon:
        return np.random.randint(n_outputs)
    else:
        return action 
</code></pre>
<p>Epsilon 应该随着经验的增多而逐渐减小，最后绝大部分动作都从经验当中得出。</p>
<h2 id="经验回放缓冲">经验回放缓冲</h2>
<p>因为每一次游戏都能获得经验，因此我们选择设计一个游戏经验池，每一次从其中取出 5 份，在训练时随机抽取。</p>
<pre><code class="language-python">buffer_len = 20000
exp_buffer = deque(maxlen=buffer_len)

def sample_memories(batch_size):
    perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]
    mem = np.array(exp_buffer)[perm_batch]
    return mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4] 
</code></pre>
<h2 id="超参数定义">超参数定义</h2>
<ul>
<li><code>num_episodes</code>：决定游玩的游戏回合数</li>
<li><code>batch_size</code>：批尺寸，过小将导致不能完全收敛，过大将占用过多的内存容量，花费的时间也会成倍增加。</li>
<li><code>input_shape / X_shape</code>：数据的输入形状</li>
<li><code>learning_rate</code>：学习率，学习率大收敛速度快，但容易出现损失值爆炸 / 震荡的情况；学习率小收敛速度慢，容易出现过拟合</li>
<li><code>discount_factor</code>：计算奖励的 gamma 值，越趋向 1 代表模型越有「远见」，但训练也越困难。</li>
</ul>
<h2 id="代码解释">代码解释</h2>
<pre><code class="language-python">logdir = 'logs'
tf.reset_default_graph()
 
# 初始化变量
X = tf.placeholder(tf.float32, shape=X_shape)
 
# 设定一个布尔值来切换「训练模式」
in_training_mode = tf.placeholder(tf.bool)
 
# 设定好主要的 Q 网络及对应的输出
mainQ, mainQ_outputs = q_network(X, 'mainQ')
 
# 目标 Q 网络也是同样的建立方法
targetQ, targetQ_outputs = q_network(X, 'targetQ')
 
# 初始化变量
X_action = tf.placeholder(tf.int32, shape=(None,))
Q_action = tf.reduce_sum(
    targetQ_outputs * tf.one_hot(X_action, 9), axis=-1, keep_dims=True)
 

copy_op = [tf.assign(main_name, targetQ[var_name])
                     for var_name, main_name in mainQ.items()]
copy_target_to_main = tf.group(*copy_op)
 
# 初始化变量，设定输出的占位符
y = tf.placeholder(tf.float32, shape=(None, 1))
 
# 损失函数，实际值与预测值两者之差（方差）
loss = tf.reduce_mean(tf.square(y - Q_action))
 
# 使用优化算法 Adam
optimizer = tf.train.AdamOptimizer(learning_rate)
training_op = optimizer.minimize(loss)
 
init = tf.global_variables_initializer()
 
# Tensorboard
loss_summary = tf.summary.scalar('LOSS', loss)
merge_summary = tf.summary.merge_all()
file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())
 
with tf.Session() as sess:
    init.run()

    for i in range(num_episodes):
        done = False
        obs = env.reset()
        epoch = 0
        episodic_reward = 0
        episodic_loss = []
 
        # 反复进行，直至游戏结束（命用完了）
        while not done:

            # 处理画面
            obs = preprocess_observation(obs)
 
            # 传入画面并获得各个动作的 Q 值
            actions = mainQ_outputs.eval(
                feed_dict={X: [obs], in_training_mode: False})
 
            # 找出 Q 值最高的动作（最佳动作）
            action = np.argmax(actions, axis=-1)
 
            # 使用 Epsilon-greedy 来决定是随机还是使用该动作
            action = epsilon_greedy(action, global_step)

            # 执行动作，并取得下一状态的画面、奖励等
            next_obs, reward, done, _ = env.step(action)

 		# 传入缓冲区，作为经验
            exp_buffer.append(
                [obs, action, preprocess_observation(next_obs), reward, done])
		# 当缓冲区里经验足够「丰富」时，开始使用这里面的取样来训练网络
            if global_step % steps_train == 0 and global_step &gt; start_steps:

                # 从缓冲区中取出一小段游戏经验
                o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(
                    batch_size)
 
                # 取出状态、下一状态
                o_obs = [x for x in o_obs]
		    o_next_obs = [x for x in o_next_obs]
 
                # 下一个动作
                next_act = mainQ_outputs.eval(
                    feed_dict={X: o_next_obs, in_training_mode: False})
 
                # 奖励
                y_batch = o_rew + discount_factor * \
                    np.max(next_act, axis=-1) * (1-o_done)
  
                # 训练模型 计算损失
                train_loss, _ = sess.run([loss, training_op], feed_dict={X: o_obs, y: np.expand_dims(
                    y_batch, axis=-1), X_action: o_act, in_training_mode: True})
                episodic_loss.append(train_loss)

  		# 隔一段时间就复制主 Q 网络的权重到目标 Q 网络上
            if (global_step + 1) % copy_steps == 0 and global_step &gt; start_steps:
                copy_target_to_main.run()

            obs = next_obs
            epoch += 1
            global_step += 1
            episodic_reward += reward

        print('Epoch', epoch, 'Reward', episodic_reward)
</code></pre>
<h2 id="运行模型">运行模型</h2>
<figure data-type="image" tabindex="4"><img src="https://cdn.mywpku.com/20200126234518.png" alt="" loading="lazy"></figure>
<p>前期效果并不理想，但在一段时间后智能体能够有意识地做出躲避怪兽、尽量吃豆得分等最大化奖励的行为。</p>
<video id="video" controls="" preload="none">
      <source id="mp4" src="https://cdn.mywpku.com/pacman.mp4"
      <p>Your user agent does not support the HTML5 Video element.</p>
</video>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[强化学习：The Taxi Problem]]></title>
        <id>https://blog.g4.cx/post/the-taxi-problem</id>
        <link href="https://blog.g4.cx/post/the-taxi-problem">
        </link>
        <updated>2020-01-26T15:24:42.000Z</updated>
        <summary type="html"><![CDATA[<p>接上一篇 <a href="https://blog.g4.cx/post/the-frozen-lake/">强化学习：The Frozen Lake</a>，同样是使用 OpenAI Gym 来玩另一个游戏：The Taxi Problem。环境和接口都大同小异。</p>
]]></summary>
        <content type="html"><![CDATA[<p>接上一篇 <a href="https://blog.g4.cx/post/the-frozen-lake/">强化学习：The Frozen Lake</a>，同样是使用 OpenAI Gym 来玩另一个游戏：The Taxi Problem。环境和接口都大同小异。</p>
<!--more-->
<figure data-type="image" tabindex="1"><img src="https://cdn.mywpku.com/20200126232510.png" alt="" loading="lazy"></figure>
<h2 id="基本参数">基本参数</h2>
<h3 id="envrender">env.render()</h3>
<ul>
<li>蓝色 B：接乘客点</li>
<li>紫色 Y：乘客下车位置</li>
<li>|：代表不能穿越的墙</li>
<li>黄块：出租车，当车上有乘客时会变绿</li>
</ul>
<h3 id="action_space-observation_space">action_space / observation_space</h3>
<ul>
<li>action_space：[0, 1, 2, 3, 4, 5] # 左下右上，接乘客，放下乘客</li>
<li>observation_space：[0, 1, … , 24] # 5x5 网格</li>
</ul>
<h2 id="q-learning-解决-taxi-问题">Q-Learning 解决 Taxi 问题</h2>
<p>对于出租车游戏来说，有以下规则：</p>
<ul>
<li>在一个地点接乘客，然后送到指定的另一个地点</li>
<li>完成一次成功载客，可以得到 20 分；将乘客送到错误地点会扣 10 分</li>
<li>出租车每移动一步，扣 1 分</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://cdn.mywpku.com/20200126233211.png" alt="" loading="lazy"></figure>
<p>Q-Learning 执行过程如下：</p>
<ul>
<li>初始化 Q 表，将所有 0 和 Q 值初始化为任意常数。</li>
<li>让智能体对环境作出反应並探索这些操作（并选取 6 种操作中最好的一种）</li>
<li>执行上一步选择的操作</li>
<li>重复，选择最好的一种操作（Q 值最高）</li>
<li>使用公式更新 Q 表值</li>
<li>不断重复，如果达到状态则结束。</li>
</ul>
<pre><code class="language-python">import gym  
import random  
    
env = gym.make('Taxi-v2')  
    
alpha = 0.4  
# 衰减因子  
gamma = 0.999  
# ε  
epsilon = 0.017  
    
# 初始化Q表  
Q = {}  
for s in range(env.observation_space.n):  
    for a in range(env.action_space.n):  
        Q[(s, a)] = 0  
    
# 更新Q表  
def update_q_table(prev_state, action, reward, nextstate, alpha, gamma):  
    # maxQ(s',a')  
    qa = max([Q[(nextstate, a)] for a in range(env.action_space.n)])  
    # 更新Q值  
    Q[(prev_state, action)] += alpha * (reward + gamma * qa - Q[(prev_state, action)])  
    
    
# ε-greedy 策略选取动作  
def epsilon_greedy_policy(state, epsilon):  
    # 随机选取一个另外的动作  
    if random.uniform(0, 1) &lt; epsilon:  
        return env.action_space.sample()  
    # 否则，选取令当前状态下 Q 值最大的动作  
    else:  
        return max(list(range(env.action_space.n)), key=lambda x: Q[(state, x)])  
    
    
for i in range(1000):  
    r = 0  
    # 初始化  
    state = env.reset()  
    while True:  
        # 采用 ε-greedy 选取动作并执行  
        action = epsilon_greedy_policy(state, epsilon)  
        nextstate, reward, done, _ = env.step(action)  
        # 将当前与上一步信息一同更新Q表  
        update_q_table(state, action, reward, nextstate, alpha, gamma)  
        # 切换到下一状态  
        state = nextstate  
        # 累加奖励  
        r += reward  
        # 当游戏达到最终状态时结束  
        if done:  
            break  
    
    print(&quot;Total reward: %d&quot; % (r))  
env.close()
</code></pre>
<h2 id="sarsa-解决-taxi-问题">SARSA 解决 Taxi 问题</h2>
<p>与 Q-Learning 流程类似，唯一不同在于更新 Q 表的方式：</p>
<figure data-type="image" tabindex="3"><img src="https://cdn.mywpku.com/20200126233156.png" alt="" loading="lazy"></figure>
<pre><code class="language-python">for i in range(1000):  
    r = 0  
    state = env.reset()  
    action = epsilon_greedy_policy(state, epsilon)  
    while True:  
        # 执行动作得到的一些信息  
        nextstate, reward, done, _ = env.step(action)  
        # 采用 ε-greedy 策略选取下一步的动作  
        nextaction = epsilon_greedy_policy(nextstate, epsilon)  
        # 更新Q表  
        Q[(state, action)] += alpha * (reward + gamma * Q[(nextstate, nextaction)] - Q[(state, action)])  
        # 切换到下一动作  
        action = nextaction  
        # 切换到下一状态  
        state = nextstate  
        # 累加奖励  
        r += reward  
        # 判断episode是否到达最终状态  
        if done:  
            break  
    print(&quot;Total reward: %d&quot; % (r))  
</code></pre>
<h2 id="q-learning-和-sarsa-的异同">Q-Learning 和 SARSA 的异同</h2>
<p>Q-Learning: 初始化 Q 值 -&gt; 开始迭代 -&gt; 选择状态中最好动作（epsilon-greedy） -&gt; 执行动作并切换到新状态，获得奖励 -&gt; 根据 Q(s,a) 更新 Q 表 -&gt; 如果不是终止状态，循环往复</p>
<p>SARSA：初始化 Q 值 -&gt; 开始迭代 -&gt; 选择状态中最好动作（epsilon-greedy） -&gt; 执行动作并切换到新状态，获得奖励 -&gt; 根据经验选择状态最好动作（epsilon-greedy） -&gt; 根据前一状态更新 Q 表 -&gt; 如果不是终止状态，循环往复</p>
<p>可以看出，Q-Learning在每一步都采用 ε-greedy 来选取最好动作，这也就导致了下一次选取的动作跟上一次没有直接关系；而SARSA会根据前一状态更新 Q 表，而后再采用 ε-greedy 来选取最好动作。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[2019-2020 跨年博客]]></title>
        <id>https://blog.g4.cx/post/2019-2020</id>
        <link href="https://blog.g4.cx/post/2019-2020">
        </link>
        <updated>2020-01-02T03:03:05.000Z</updated>
        <summary type="html"><![CDATA[<p>写跨年博客的想法其实很早就有了，但每到真正下笔时却总是半途而废。再忙也好、再晚也好，我今天说什么都要把这篇博客写完推到网上。</p>
]]></summary>
        <content type="html"><![CDATA[<p>写跨年博客的想法其实很早就有了，但每到真正下笔时却总是半途而废。再忙也好、再晚也好，我今天说什么都要把这篇博客写完推到网上。</p>
<!--more-->
<p>人们常说人生有三个转折点：出生、高考、婚嫁。如果说 2017-2018 让我对人生重新做了选择，那么 2019 年对我来说，无疑是检验这次选择的关键一年。</p>
<h2 id="2019">2019</h2>
<h3 id="生活">生活</h3>
<p>在杭电的生活一定能算得上是丰富而多元的。我看了不下 20 场的文艺演出和学术讲座，闲时有也常去周边的城市逛逛：从南京到上海，再到苏州和武汉……眼界着实开阔了不少，看到了平静村落下的朴实善良，也看到繁华都市里的「熙熙攘攘」。</p>
<p><img src="https://cdn.mywpku.com/IMG_20191026_141537.jpg" alt="" loading="lazy"><br>
<em>苏州街头的小桥流水</em></p>
<p>学校离西湖景区只有 40 分钟地铁，一年下来我也只走过 1/10 吧，嗯毕业前一定会打卡每个景点的！</p>
<p><img src="https://cdn.mywpku.com/IMG_20190615_082056.jpg" alt="" loading="lazy"><br>
<em>9 教随手拍</em></p>
<p>第一次去看演唱会，那种和 idol 近在咫尺的感觉真的很棒！陈绮贞、逃跑计划、万青、林宥嘉、岸部真明（我和岸部大叔面对面！！）</p>
<p><img src="https://cdn.mywpku.com/IMG_20191206_212447.jpg" alt="" loading="lazy"><br>
<em>岸部真明 x 下山亮平 2019 杭州演奏会</em></p>
<p>我的舍友都超级 nice，好多小细节上能体现出他们的修养（不像我这种憨憨直男哈哈哈），每到学期末和重要日子还会很仪式性地出去搓一顿。（也挺好玩的哈哈哈，过了快半年了才互相发现原来 5 个里有 3 个都是复读生</p>
<p>每一次回家总能感觉到家人在慢慢变老。外公外婆这两年身体每况愈下，虽然生活尚能自理，但总感觉他们已被阿兹海默症的洪水冲毁了常识和心智，也因此遗失了感情。</p>
<p>我很希望我能做些什么，真的，不想再看到我妈哭了。</p>
<h3 id="社团">社团</h3>
<p>参加了杭电助手和红色家园两个重技术型的社团（也不出所料地成为了两家之间的双面间谍 溜了溜了），认识了很多很多这方面的牛人和志同道合的朋友，甚至还借社团的名义帮学校做了好多事情（学院的官网、微信新生报到、重构各类外包系统等等），这些宝贵的人脉和经历，我想只有助手和红家能带给我，只有杭电能带给我。</p>
<p>原本只想做一个闷头实现功能写代码的人，谁知阴差阳错还是当上了前端的部长。从最初的线上宣传，一个一个和我「未来」的部员们聊过去，到组织笔试、二面、三面，再到每一周组织例会……所幸，这一年里我的表现（和魅力）还算说得过去，大家都很认真很可爱，不仅没有像我那时的前端部一样没到几个月便隐居山林不再出现，还很积极地完成我给的各种作业和社团任务，有的甚至还和我在办公室里熬夜复习，这对我来说已经是莫大的肯定了。</p>
<p><img src="https://cdn.mywpku.com/IMG_20190708_221413.jpg" alt="" loading="lazy"><br>
<em>助手的🐱（现在已经放生了，想它）</em></p>
<p>社团的开发工作和部门上的这些事情大概占据了我超过一半的空余时间，真的很累很累，但不得不说，这段日子里我真的，真的超级开心。</p>
<p><img src="https://cdn.mywpku.com/IMG_20191125_205430.jpg" alt="" loading="lazy"><br>
<em>开会偷拍我的部员们</em></p>
<p><img src="https://cdn.mywpku.com/Screenshot_20200102-015934.jpg" alt="" loading="lazy"><br>
<em>助手破冰，跳失恋阵线联盟</em></p>
<h3 id="竞赛和创业">竞赛和创业？</h3>
<p>因为总想着要出国，大大小小的比赛也报了不下 10 场。挑战杯，互联网+、服务外包、大创新苗……每一次打比赛都让人觉得特别不舒服，不仅仅是要在规定时间内交上足够满意的结果所带来的一阵阵焦虑，更有因为团队管理上的缺乏经验而自卑，和生怕辜负队友、老师努力和期待的强烈不安。</p>
<p>原本只是无心插柳，可歪打正着地遇上了学校创业类的金牌指导老师老吴，他不仅仅是在比赛技巧上，更从我的发展方向，人生角度上给了我很多经验和思考。</p>
<p>第一次认识老吴时总觉得他看起来像个挺不靠谱的老师，比如什么忘记了报名的时间、校赛那天没注意我们讲的什么，但他一到真正的关键时刻却只能让你从内心发出由衷的佩服，你也不知道这小小的身躯里到底装了多少的知识和才华。</p>
<p><img src="https://cdn.mywpku.com/IMG_20190710_111723.jpg" alt="" loading="lazy"><br>
<em>参加第五届省互联网+，输的超惨哈哈</em></p>
<p>他还总能给我带来许多感动（有一次甚至还和老婆孩子打电话说今晚辅导学生不回去了）。我总记得互联网+比赛失利那天，回想起队友为了这次比赛推掉了原本安排好的行程，努力了这么久最后却落得小组倒数的成绩，我心有不甘，更多的是愧疚和自责。要知道悲伤同样可以被传染，我又怎敢向队友再分享这种负面的情绪呢。我发短信给老吴，为我的状态不佳向他道歉时——我第一次听到，听到一个以老师身份的人，能耐下心来接受我所有的不安和焦虑，还对我这样保证。</p>
<p>「我今后一定帮你把企业做大做强。」</p>
<p>我实在没有勇气和他说创办公司只为了打比赛这回事。</p>
<p>可我每每想起那段时光，眼里总噙泪水。</p>
<h3 id="感情">感情</h3>
<p>很遗憾，9102 年我还是没脱单。不过今年是和往常不同，收到过不明所以的短信、也从微博上无意翻到别人对我的评价，甚至让我还差点动了内销的念头……</p>
<p>这一年里，似乎多了好几个拷问自己的夜晚，具体问的什么实在是没脸说。</p>
<p>之前崇拜和喜欢的人脱单了<s>npy还挺帅的</s>，哎难过了好一段时间，特别是偶遇时，我心里真的好复杂好复杂……心想还要做一段时间队友，更复杂了……</p>
<h2 id="2020">2020</h2>
<p>我没想到 20 岁来的这么快，我也真正到了要面对些什么的年纪。先定个小计划：</p>
<ul>
<li>~<s>把谷歌 STEP 项目的简历投过去</s>~ 被拒了</li>
<li>考下雅思或托福，学车</li>
<li>做一个拿得出手的开源项目</li>
<li>跟着导师还是得做些什么的对吧</li>
<li>把极客苏打做出来</li>
<li>去报吉他班，自己练真的没啥进步</li>
<li>少油少甜戒糖戒油炸，虽然我知道肯定不会执行</li>
<li>去一次日本或 HK（再乱也想去看看）</li>
<li>看一场彩虹合唱团的演出</li>
<li>多陪陪家人，外公外婆真的老了</li>
</ul>
<p>当然了，思想上的一点点要求也得说说：</p>
<ul>
<li>保持谦恭，对于我不知道的世界不应过多评价</li>
<li>DONT WORRY，生死面前皆为小事</li>
<li>BE OPEN，早点离开自己的那些固化思维，别一上来和你想的不一样就妄加批判</li>
<li>靠谱，凡事有交代、件件有着落、事事有回音</li>
<li>温热，别再被谁说是高冷怪了</li>
</ul>
<p>最重要的可能还是：</p>
<ul>
<li>改变这种一个人也能活的笨蛋思想</li>
<li>早点认清楚错过才不是什么美好，错过就是怂 + 自我安慰给你下的慢性毒药</li>
<li>赶紧脱单</li>
</ul>
<p>还是很喜欢副部卡片上写的那八个字，虽然看起来平淡无力，但确实是我所能想到的，最棒的祝福：</p>
<p>万事胜意，平安喜乐。</p>
<p>2020 年祝好，晚安。</p>
<p>2020.01.02 01:35</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[初入强化学习]]></title>
        <id>https://blog.g4.cx/post/3Q3S9-46P</id>
        <link href="https://blog.g4.cx/post/3Q3S9-46P">
        </link>
        <updated>2019-11-06T14:50:56.000Z</updated>
        <summary type="html"><![CDATA[<p>原作 Medium 地址：https://medium.com/@adeshg7/introduction-to-reinforcement-learning-part-1-dbfd19c28a30</p>
<p>翻译文章式学习，感觉没有比我更奇葩的了……看看翻完能不能搞懂这些。</p>
]]></summary>
        <content type="html"><![CDATA[<p>原作 Medium 地址：https://medium.com/@adeshg7/introduction-to-reinforcement-learning-part-1-dbfd19c28a30</p>
<p>翻译文章式学习，感觉没有比我更奇葩的了……看看翻完能不能搞懂这些。</p>
<!--more-->
<h2 id="强化学习是什么">强化学习是什么？</h2>
<blockquote>
<p>强化学习是一套帮你做决策的科学</p>
</blockquote>
<p>我们在玩马里奥、战地这类通关式游戏的时候总要不断重复这样的步骤：</p>
<blockquote>
<p>观察环境 → 做出行动 / 决策 → 分析刚刚做的决策是好是坏</p>
</blockquote>
<p>就像你打马里奥一样，打多了输多了自然不会在同一个陷阱里摔两次。不断重复这样的步骤实际上在帮助我们记住在什么情况下应该 / 不应该做出哪样的行动，并且把这些行动串起来，怎样才能拿到游戏的最高分。</p>
<h2 id="强化学习类问题">强化学习类问题</h2>
<p>强化学习类问题大概长这样：</p>
<figure data-type="image" tabindex="1"><img src="https://cdn.mywpku.com/20191106225823.png" alt="" loading="lazy"></figure>
<h3 id="名词解释">名词解释</h3>
<p><strong>奖励（Rewards）：</strong></p>
<p>说是奖励不如说是得分，这就是衡量游戏玩的好不好的关键，强化学习最终也是为了让所获得的奖励尽量最大。</p>
<p>举个例子，你在玩坦克大战，得分和扣分可能是这样的：</p>
<ul>
<li>成功击中敌方坦克：<strong>+ 1 分</strong></li>
<li>敌方坦克被摧毁：<strong>+ 10 分</strong></li>
<li>你 / 你的队友被打了：<strong>- 1 分</strong></li>
<li>你的队友被打死了：<strong>- 10 分</strong></li>
</ul>
<p><strong>目标：</strong></p>
<p>通过分析，做各种判断和行动的形式让所获得的奖励尽量最大。不过奖励也有分短期和长期，田忌赛马了解一下。</p>
<p><strong>状态：</strong></p>
<p>指的是当前环境下的信息，这些信息一般是有助于去预测下一个状态是咋样的。</p>
<p><strong>智能体（Agent）状态和环境状态：</strong></p>
<p>智能体就代表一个玩家，玩家的状态多数只有他自己知道，状态的信息有助于强化学习去选择下一项操作——比如说你现在空血没魔，技能都还在 CD，这时候你选择 TP 泉水还是自信回头？</p>
<p>环境会根据智能体所作出的行动来发生变化。不过环境的状态智能体不一定会知道，甚至有些情况下环境状态没啥用处。</p>
<p><strong>马尔科夫状态：</strong></p>
<p>马尔科夫状态（The Markov state）想告诉我们的就是：它认为<strong>未来</strong>只和<strong>当下</strong>有关，和历史无关。比如说你在跑 50 米，你下一秒摔不摔跤只和你<strong>当下</strong>有没有被石头绊倒或是踩到了自己的鞋带有关，至于从啥时候开始跑的跑前心情咋样通通都是历史。</p>
<p>马尔可夫状态真的很妙，它蕴藏的哲学可能是「历史塑造了我」，你的当下就是历史的积淀——我之前做过什么、喜欢什么、环境曾经带给我什么都会或多或少的呈现在现在的我身上，至于其他的都是无用信息，与决策无关。</p>
<figure data-type="image" tabindex="2"><img src="https://cdn.mywpku.com/20191106232940.png" alt="" loading="lazy"></figure>
<p>用马里奥来总结一下这些名词大概就是：</p>
<ul>
<li><strong>环境</strong>：整个马里奥世界（水管、砖块、路上的金币 blabla）</li>
<li><strong>状态</strong>：世界里某一个时间点的缩影（屏幕上只能显示世界的一小块）</li>
<li><strong>智能体</strong>：你控制的马里奥</li>
<li><strong>行为 / 动作</strong>：跳、蹲、向左、向右、发射（喷火马里奥有这个）</li>
<li><strong>奖励</strong>：吃到 🍄了、顶到金币了都可以是奖励，其中金币就是瞬时奖励。</li>
<li><strong>短期目标</strong>：成功降旗到了🏰</li>
<li><strong>长期目标</strong>：打败了大魔王救出了公主</li>
</ul>
<p><strong>全知境界（Fully Observable Environment）：</strong></p>
<p>意思就是智能体本身就拥有「上帝视角」，这又叫马尔科夫过程（Markov Decision Process）</p>
<pre><code>智能体状态 = 环境状态 = 马尔科夫状态
</code></pre>
<p><strong>未知境界（Partially Observable Environment）：</strong></p>
<p>智能体需要靠之前的经验来一步步获取环境的各种信息，这又叫部分可知马尔科夫过程（Partially Observable Markov Decision Process）。就像 Dota 里的英雄没法知道整个地图的情况一样，除非你自己走到那里或者买眼。</p>
<pre><code>智能体状态 ≠ 环境状态
</code></pre>
<h2 id="强化学习智能体的组成部分">强化学习智能体的组成部分</h2>
<p><strong>策略：</strong></p>
<p>策略就是智能体整体的行为，它是一套能够让智能体得最高分的规则：抽象来讲就是给状态和动作建立联系。</p>
<p><strong>值函数：</strong></p>
<p>值函数是针对一个状态预期奖励（得分）的预测。比如现在走左边和走右边会有两种不同结局，那要怎么知道哪种结局是最好的呢？值函数量化了每种状态，让你的智能体知道应该往哪走。</p>
<figure data-type="image" tabindex="3"><img src="https://cdn.mywpku.com/20191107092104.png" alt="" loading="lazy"></figure>
<p>整个公式得出的就是预测值，值 <strong>v</strong> 由参数 <strong>状态 s</strong> 决定，<strong>状态 s</strong> 遵循着 <strong>决策 𝛑</strong>、<strong>时刻 t</strong> 是指当前状态所处的时刻，下一秒则是 <strong>t + 1</strong>。</p>
<p>Gamma, <strong>𝛄 系数</strong> 在我们关注当前 / 之后的状态时会起作用。如果只关注当前状态——<code>𝛄 = 0</code>；如果关注所有相关联的状态——<code>𝛄 = 1</code>，值越接近 1，状态的关联性越强。</p>
<p><strong>模型：</strong></p>
<p>模型是环境下智能体的表现，它能分成两种状态：</p>
<p>过渡：预测着下一个状态。比如给出小车的速度和位置，得出「切换」到下一个状态的概率。</p>
<figure data-type="image" tabindex="4"><img src="https://cdn.mywpku.com/20191107093233.png" alt="" loading="lazy"></figure>
<p>奖励：预测的是瞬时奖励，当智能体做出动作时返回它所获得的奖励值。</p>
<figure data-type="image" tabindex="5"><img src="https://cdn.mywpku.com/20191107093449.png" alt="" loading="lazy"></figure>
<p>不一定要构建模型，强化学习中也有很多高效的算法选择不使用模型这套理论。</p>
<h2 id="强化学习智能体的各种类型">强化学习智能体的各种类型</h2>
<figure data-type="image" tabindex="6"><img src="https://cdn.mywpku.com/20191107093658.png" alt="" loading="lazy"></figure>
<ul>
<li>值驱动：没有确定的策略但却有非常直观的值函数，所有动作都取决于让值函数取到最大值。</li>
<li>策略驱动：有非常直观的策略但却没有确定的值函数，它关注的是 It sees how well it does by picking some actions and how much reward it gets.</li>
<li>无模型：没有模型意味着智能体不知道环境是怎么运作的，也没有确定的值函数和策略</li>
<li>模型驱动：使用模型及策略 / 值函数</li>
<li>Actor-critic：同时使用策略 + 值函数</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[强化学习：冰湖游戏 The Frozen Lake]]></title>
        <id>https://blog.g4.cx/post/the-frozen-lake</id>
        <link href="https://blog.g4.cx/post/the-frozen-lake">
        </link>
        <updated>2019-11-05T03:23:44.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<p>冰湖的游戏规则是这样的，类似于迷宫：</p>
<ul>
<li>冰湖总共有 16 个小格（4x4），小格有两种类型：冰面（F）和冰窟（H）</li>
<li>从起点（S）出发，到达终点（G）或掉入冰窟（H）为游戏结束</li>
<li>你控制着一个小人，在冰面（F）上可以按四个方向移动，用数字代表移动方向（左 0 / 下 1 / 右 2 / 上 3）</li>
<li>你的目的是到达终点，而不是掉进洞里</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://cdn.mywpku.com/20191105113330.png" alt="" loading="lazy"></figure>
<p>为了解释方便，我们再来给迷宫掰掰细节：</p>
<ul>
<li><code>状态（State）</code>：给每个小格做编号（0~15），例如第一行第三列编号为 2，第二行第一列编号为 4，以此类推</li>
<li>移动后可能会去往另一个冰面——当然咯，如果你已经掉到洞里或者已经到达终点了：不管怎么移动都不会再改变状态</li>
<li>在边界位置移动可能会撞到墙——撞墙了不会改变状态</li>
</ul>
<h2 id="搭个环境">搭个环境</h2>
<pre><code class="language-python">import gym
import numpy as np

env = gym.make('FrozenLake-v0', is_slippery=False)
</code></pre>
<p>执行 <code>env.render()</code> 后可查看当前冰湖的状态：</p>
<pre><code class="language-python">env.reset() # 重置环境           
env.render() # 输出当前环境
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://cdn.mywpku.com/20191105115716.png" alt="" loading="lazy"></figure>
<p>以 <code>红色背景</code> 标记当前小人的位置。<code>env.action_space</code> 和 <code>env.observation_space</code> 返回的是 <code>Discrete</code> 类型，它们都描述大小为 <code>n</code> 的离散空间：</p>
<pre><code class="language-python">print(&quot;Action space: &quot;, env.action_space) # Discrete(4) 意味着可执行四种动作
print(&quot;Observation space: &quot;, env.observation_space) # Discrete(16) 意味着空间大小为 16
</code></pre>
<p>为了能让小人移动，我们还需要用到两个 API：<code>env.action_space.sample()</code> 和 <code>env.step()</code></p>
<pre><code class="language-python">random_action = env.action_space.sample()
print(random_action) # 返回 0, 1, 2, 3 其中之一，代表移动的方向

new_state, reward, done, info = env.step(random_action) # 输入函数，操作后将按照所给方向移动
# 返回四个值的 Tuple：
# new_state: 移动后所在的状态（位置）
# reward: 获得的奖励（目前始终为 0，不涉及）
# done: 布尔值，代表游戏是否结束（掉坑里或者走到终点了返回 True）
# info: 调试用
</code></pre>
<p>知道这些之后，可以让小人随便走几步，每走一步输出一次当前环境的信息：</p>
<pre><code class="language-python">env.reset() # 走之前先重置环境
MAX_ITERATION = 10 # 走 10 步

for i in range(MAX_ITERATION):
    random_action = env.action_space.sample()
    print(env.step(random_action))
    env.render()
</code></pre>
<figure data-type="image" tabindex="3"><img src="https://cdn.mywpku.com/20191105121029.png" alt="" loading="lazy"></figure>
<h2 id="值迭代">值迭代</h2>
<p>对于冰湖游戏来说，在 4x4 的网格上有以下规则：</p>
<ul>
<li>不能掉到窟窿里，否则游戏结束（奖励 - 1）</li>
<li>在行走过程中没有奖励</li>
<li>当做出的动作超出边界时，会保持原地不动，没有奖励</li>
<li>（is_Slippery=True 时）冰面上执行动作会有 1/3 的概率打滑，从而向其它的三个方向移动</li>
</ul>
<p>为了简化整个值迭代叙述的流程，先假设路途中没有窟窿、也不会有打滑的现象发生：</p>
<figure data-type="image" tabindex="4"><img src="https://cdn.mywpku.com/20200126231854.png" alt="" loading="lazy"></figure>
<p>以马里奥找宝箱为例（和冰湖大同小异，图就不做了……），每一个位置（状态）上都有着自己所对应的值，值的不断迭代（变化）由贝尔曼方程决定：</p>
<figure data-type="image" tabindex="5"><img src="https://cdn.mywpku.com/20200126231907.png" alt="" loading="lazy"></figure>
<p>唯一的区别是冰湖系统中行走没有奖励，上图每行走一步需要耗费一点体力（Reward - 1）</p>
<p>以找宝箱为例，迭代过程如下：</p>
<ul>
<li>初始化：所有位置值初始化为 0</li>
<li>在每个位置上都尝试一遍上下左右，并且记录下每个动作带来的奖励以及新位置（状态）。这时根据奖励的多少得出选择最优的Action，更新V(s) = Reward + V(s') = -1 + 0。因为第一轮从任何位置移动都会耗费一点体力，所以除了宝箱位置（走到宝箱游戏结束）之外所有位置（状态）都为 -1.</li>
<li>第二轮迭代：重复第一步操作，这时对于宝箱周围的位置（State），最优的行为当然是走到宝箱位置，所以此时离宝箱最近的几个位置值最大，V(s) = Reward + V(s') = -1 + 0</li>
<li>第三轮迭代：重复第二步操作，此时取了最佳动作之后倘若还没有走到宝箱处的 State 再 – 1，这时距离最远的两个位置（State） 值最小，离宝箱最近的值最大，这时迭代已找到最优策略：只需要往损失最小的方向行走，就一定能找到宝箱。</li>
</ul>
<p>冰湖游戏也类似。</p>
<h2 id="策略迭代">策略迭代</h2>
<p>值迭代是根据行进过程中不断探索得出了最佳的策略；策略迭代则是反过来：先生成一条路线，然后评估它够不够好——如果不够那就再来一次，直至评估值趋向不变为止。</p>
<p>还是以简化后的系统马里奥找宝箱为例：</p>
<figure data-type="image" tabindex="6"><img src="https://cdn.mywpku.com/20200126232016.png" alt="" loading="lazy"></figure>
<p>策略迭代公式：</p>
<figure data-type="image" tabindex="7"><img src="https://cdn.mywpku.com/20200126232030.png" alt="" loading="lazy"></figure>
<p>策略迭代过程如下：</p>
<ul>
<li>初始化：随机选择策略，比如一直向下走</li>
<li>策略评估：计算V(s)，如果宝藏恰好在正下方，则期望价值等于到达宝藏的距离(-2或者-1)；如果宝藏不在正下方，则永远也不可能找到宝藏，期望价值为负无穷。</li>
<li>策略提升：根据V(s) 找到更好的策略，如果宝藏不在正下方，根据可以得出最优策略为横向移动一步</li>
<li>迭代：对上一步继续执行策略评估和提升，直至策略不再变化为止。</li>
</ul>
<p>可以看出策略迭代和值迭代最终所生成的 Q Table 应是一致的。</p>
<h2 id="代码解释">代码解释</h2>
<pre><code class="language-python">import gym  
import numpy as np  
env = gym.make('FrozenLake-v0')  
    
def value_iteration(env, gamma = 1.0):  
    value_table = np.zeros(env.observation_space.n)  
    # 迭代次数  
    no_of_iterations = 100000  
    # 阈值  
    threshold = 1e-20  
    
    for i in range(no_of_iterations):  
        updated_value_table = np.copy(value_table)  
        
        for state in range(env.observation_space.n):  
            Q_value = []  
    
            for action in range(env.action_space.n):  # 遍历所有可能转移的状态  
                next_states_rewards = []  
                for next_sr in env.P[state][action]:  
                    trans_prob, next_state, reward_prob, _ = next_sr  
                    # 下一状态 t 的动作状态价值 = 转移到 t 状态的概率 × （ 环境反馈的 reward + γ × t 状态的当前价值 ）  
                    next_states_rewards.append((trans_prob * (reward_prob + gamma * updated_value_table[next_state])))  
                # 将某一动作执行后，将所有可能的 t+1 状态的价值加起来   
                Q_value.append(np.sum(next_states_rewards))  
            # 选 Q_value 的最大值作为该状态的价值  
            value_table[state] = max(Q_value)  
    
            # 如果已经非常接近理想值（相差小于阈值），则终止循环返回值表  
            if (np.sum(np.fabs(updated_value_table - value_table)) &lt;= threshold):  
                print ('Value-iteration converged at iteration# %d.' %(i+1))  
                break  
    return value_table  
    
def extract_policy(value_table, gamma = 1.0):  
    policy = np.zeros(env.observation_space.n)  
    # 与值迭代不同，策略迭代不更新值函数，而是选出每个状态下对应最大价值的动作  
    for state in range(env.observation_space.n):  
        Q_table = np.zeros(env.action_space.n)  
        for action in range(env.action_space.n):  
            for next_sr in env.P[state][action]:  
                trans_prob, next_state, reward_prob, _ = next_sr  
                Q_table[action] += (trans_prob * (reward_prob + gamma * value_table[next_state]))  
                policy[state] = np.argmax(Q_table)  
    return policy  
    
optimal_value_function = value_iteration(env=env,gamma=1.0)  
optimal_policy = extract_policy(optimal_value_function, gamma=1.0)  
    
print(optimal_policy) 
</code></pre>
<p></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nicechord - 终止式和副属和弦]]></title>
        <id>https://blog.g4.cx/post/nicechord-104</id>
        <link href="https://blog.g4.cx/post/nicechord-104">
        </link>
        <updated>2019-07-28T09:14:32.000Z</updated>
        <summary type="html"><![CDATA[<p>终止式指的是乐句或一首曲子「最后几个和弦」的安排方式。</p>
<p>之前提到的<a href="https://blog.g4.cx/post/nicechord-102/">和弦功能</a>当中有讲，不稳定的和弦会有接到稳定和弦的趋势——也就是从「外面」回到「家」。</p>
]]></summary>
        <content type="html"><![CDATA[<p>终止式指的是乐句或一首曲子「最后几个和弦」的安排方式。</p>
<p>之前提到的<a href="https://blog.g4.cx/post/nicechord-102/">和弦功能</a>当中有讲，不稳定的和弦会有接到稳定和弦的趋势——也就是从「外面」回到「家」。</p>
<!-- more -->
<h3 id="正格终止">正格终止</h3>
<blockquote>
<p>G 即 5 级和弦，大调中的第三个正三和弦，任何一首歌曲都不可缺少。它起着对主和弦支撑的作用。乐曲的终止感就是由 5-1 这样的进行产生的。当然现代流行音乐特别是欧美音乐中不使用 5-1 终止的歌曲也很多，这正是流行音乐的特色，但 5 级和弦作为音乐的骨架和弦仍然不可动摇。</p>
</blockquote>
<p>正格终止就是由五级和弦（V）接到一级和弦（I）的终止。也就是在 C 大调当中的 <strong>G(7) -&gt; C</strong></p>
<h3 id="半终止">半终止</h3>
<p>既然五级和弦给人非常不舒服、不稳定的感觉，那么最后停在五级和弦（V）上的终止给人的自然是一种「意犹未尽、还没结束」的感觉。</p>
<h3 id="其它">其它</h3>
<ul>
<li><strong>假终止：</strong> 本来应该是 V -&gt; I 的进行，却使用了其它方法及结束整个乐句。古典音乐常用 V -&gt; VI，有意想不到的悲伤、忧郁效果。</li>
<li><strong>变格终止：</strong> 「教会版」终止，特指 IV -&gt; I 的进行，因为「阿门」的念法刚好和 IV -&gt; I 一样，所以又叫做「阿门终止」。</li>
</ul>
<h2 id="副属和弦">副属和弦</h2>
<p>「副属和弦」（Secondary Dominant Chord）被用在两个和弦之间，起到「装饰」作用，让乐句听起来不单调，有起伏感。例如（加粗的为副属和弦）：</p>
<ul>
<li>C -&gt; <strong>B7</strong> -&gt; Em</li>
<li>C -&gt; <strong>E7</strong> -&gt; Am</li>
<li>C -&gt; <strong>D7</strong> -&gt; G</li>
<li>C -&gt; <strong>A7</strong> -&gt; Dm</li>
</ul>
<p>以上副属和弦都符合一个规则：「是从要接的那個和弦算起，往上五度音的屬七和弦」。<code>E（Em）</code> 往上五度音是 <code>B</code>，属七和弦就是 <code>B7</code>。</p>
<p>是不是有点像「终止式」当中 <strong>V -&gt; I</strong> 的关系？</p>
<p>那如果想要再装饰一下「副属和弦」怎么办？既然有了「外面」、有了「家🏠」，自然也有「桥」和弦：</p>
<figure data-type="image" tabindex="1"><img src="https://cdn.mywpku.com/20190728180647.png" alt="" loading="lazy"></figure>
<p>一般来说，在 V 和弦之前可以加入装饰性的二级和弦（「桥」和弦）。例如 <strong>C -&gt; Fmaj7</strong>：</p>
<p>加入「副属和弦」就是 <strong>C -&gt; C7 -&gt; Fmaj7</strong></p>
<p>再加入一个二级的装饰和弦：<strong>C-&gt;Gm(7)-&gt;C7-&gt;Fmaj7</strong>（F 大调的二级和弦是 Gm）</p>
<blockquote>
<p>上方：指比某一音更高的音，下方反之</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nicechord - 音程名称]]></title>
        <id>https://blog.g4.cx/post/nicechord-103</id>
        <link href="https://blog.g4.cx/post/nicechord-103">
        </link>
        <updated>2019-06-29T02:00:22.000Z</updated>
        <summary type="html"><![CDATA[<p>音程就是指「音与音之间的距离」，就像尺子有刻度一样——音和音之间也有同样固定的标准。</p>
]]></summary>
        <content type="html"><![CDATA[<p>音程就是指「音与音之间的距离」，就像尺子有刻度一样——音和音之间也有同样固定的标准。</p>
<!-- more -->
<h2 id="度">度</h2>
<figure data-type="image" tabindex="1"><img src="https://cdn.mywpku.com/20190629100213.png" alt="" loading="lazy"></figure>
<p>例如 <strong>do-re-mi</strong>，从 <strong>do</strong> 到 <strong>mi</strong> 就是 <strong>3 度</strong>。</p>
<p>度一定是 <strong>整数</strong>，这也意味着实际上对音进行升降不会影响到音程。不管是 <strong>降 mi</strong> 还是 <strong>升 mi</strong>，只要它仍叫做 <strong>mi</strong>，那么从 <strong>do</strong> 到 <strong>mi</strong> 就还是 <strong>3 度</strong>。</p>
<p>那问题来了：既然降音和升音都是一样的，那它们之间的距离怎么表示啊？</p>
<h2 id="纯音程和大音程">纯音程和大音程</h2>
<figure data-type="image" tabindex="2"><img src="https://cdn.mywpku.com/20190629100955.png" alt="" loading="lazy"></figure>
<p>在大调音阶里，每一个音和第一个音之间的距离，都叫做「完全」或「大」音程。我们把「度」分为两组：</p>
<ul>
<li>1 4 5 8 纯（完全）音程（减 ⬅️完全 ➡️增）</li>
<li>2 3 6 7 大音程（减 ⬅️小 ⬅️大 ➡️增）</li>
</ul>
<figure data-type="image" tabindex="3"><img src="https://cdn.mywpku.com/20190629102333.png" alt="" loading="lazy"></figure>
<p>将 C 大调 <strong>do-re-mi-fa-sol-la-ti-do</strong> 和第一个音 <strong>do</strong> 来比较就会像上面这样👆。</p>
<p>举个 🌰，只要音之间的距离为 <strong>4 度</strong>，那么就属于「纯音程」，读作 <strong>纯 4 度</strong>。那么，比原先音程的距离要 <strong>窄</strong> 或者 <strong>宽</strong> 的就用其它的形容词来表示。</p>
<p>举个 🌰，<strong>do-降 mi</strong> 之间是 <strong>3 度</strong>，但 <strong>降 mi</strong> 比原本的音程要窄一个单位，那么就是 <strong>小 3 度</strong>。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nicechord - 和弦功能]]></title>
        <id>https://blog.g4.cx/post/nicechord-102</id>
        <link href="https://blog.g4.cx/post/nicechord-102">
        </link>
        <updated>2019-06-27T03:14:55.000Z</updated>
        <summary type="html"><![CDATA[<p>和弦的功能我想了想，似乎都可以归到人们常说的<strong>起承转合</strong>四个字当中。</p>
]]></summary>
        <content type="html"><![CDATA[<p>和弦的功能我想了想，似乎都可以归到人们常说的<strong>起承转合</strong>四个字当中。</p>
<!-- more -->
<figure data-type="image" tabindex="1"><img src="https://cdn.mywpku.com/20190627120058.png" alt="" loading="lazy"></figure>
<p>我们来假定现在一首曲目只由单音的 <strong>do-re-mi-fa-sol sol-fa-mi-re-do</strong> 组成，前半段的进行给人一种从 <strong>do</strong> 出发一直往上爬的感觉，情绪逐渐上涨达到最高。可如果从中间断开——不弹奏后面的 <strong>sol-fa-mi-re-do</strong> 总是让人觉得不太舒服，就像是爬山「走到顶端」就呆在那不下来了一样。</p>
<p>把 <strong>do-re-mi-fa-sol-la-ti</strong> 映射到对应的和弦上来会是这样：</p>
<figure data-type="image" tabindex="2"><img src="https://cdn.mywpku.com/20190627121518.png" alt="" loading="lazy"></figure>
<p>固定地，1 级和弦称主和弦、4 级和弦可称下属和弦，5级和弦也称属和弦。</p>
<figure data-type="image" tabindex="3"><img src="https://cdn.mywpku.com/20190627123639.png" alt="" loading="lazy"></figure>
<p>了解这些有什么用呢？我们所说的谱曲和写歌，背后「和弦进行」的基本逻辑，就是从稳定的和弦再到不稳定的和弦，最后再到稳定的和弦。就好像是从「家」出发，到了杂乱的「外面」，跨桥经历千山万水之后终于再一次回「家」一样。</p>
<figure data-type="image" tabindex="4"><img src="https://cdn.mywpku.com/20190627124402.png" alt="" loading="lazy"></figure>
<p>这也就是主和弦、下属和弦和属和弦的功能，合在一起就是完成了「起  ➡️ 承  ➡️ 转」最后再回到「合」<br>
的进行。</p>
<blockquote>
<p>C 即 1 级和弦，是用来明确调性的。一般大调的歌曲都以它开始，也以它结束。不过在曲子的中间可以尽量少用主和弦，否则老是给人以终止感，乐曲的进行也会很硬。</p>
</blockquote>
<blockquote>
<p>Dm 即 2 级和弦，是一个很柔和的和弦，它的最重要用途就是放在属和弦即 5 级和弦之前。而 5 级和弦则自然要回到1级和弦，所以很容易就形成了2-5-1的进行。这是一个极其常用的进行。</p>
</blockquote>
<blockquote>
<p>Em 即 3 级和弦，也是一个十分柔和的和弦。音乐的进行中有了她马上就会变得柔美而略带忧伤。1-3-4 的进行，也即在 C 大调中的 C-Em-F 是一个很常用的进行。乐曲中本来用 1 级和弦的地方有时可以考虑换成 3 级和弦，音乐立即就不强硬了。港台音乐中这种手法很常用。</p>
</blockquote>
<blockquote>
<p>F 即 4 级和弦，大调中的又一正三和弦，属于骨干和弦之一。它十分明亮，让人感觉心胸开阔，有一种一下子「飞」起来的感觉。我们听到的美国乡村乐和描写西部大草原和大峡谷的歌曲都使用 4 级和弦来表现。1 级和弦后面跟 4 级与跟 3 级和弦是绝对不同的。</p>
</blockquote>
<blockquote>
<p>G 即 5 级和弦，大调中的第三个正三和弦，任何一首歌曲都不可缺少。它起着对主和弦支撑的作用。乐曲的终止感就是由 5-1 这样的进行产生的。当然现代流行音乐特别是欧美音乐中不使用 5-1 终止的歌曲也很多，这正是流行音乐的特色，但 5 级和弦作为音乐的骨架和弦仍然不可动摇。</p>
</blockquote>
<blockquote>
<p>Am 即 6 级和弦，一个中性的和弦，如果把它作为主和弦那就是小调了。歌曲肯定会变得忧郁，悲伤。如果 6 级和弦出现在大调中的某些部分，那它起到的就是连接不同和弦的作用。6 级和弦象一座桥，它前面可以接几乎所有的和弦，后面也是如此。它可以是和弦的进行连贯，不呆板。1-6-4-5 是极为常用的进行，事实上就用这四个和弦就可以写歌了。</p>
</blockquote>
<blockquote>
<p>Bdim 即 7 级和弦，在流行音乐中很少用。因为它是减三和弦，有一种向里收缩的紧张感，一般只在某些特定进行中使用，或是为了根音的流动而使用。</p>
</blockquote>
<blockquote>
<p>Via: <a href="https://www.douban.com/group/topic/13429306/">【乐理】和弦的功能、变化、连接及编配</a></p>
</blockquote>
<p>所以，与其给每一个和弦都定义一个功能，倒不如把现有的所有和弦都归类到「起、承、转」当中，只要遵循着最简单的 TDS 公式来谱曲就会很好听啦！</p>
<figure data-type="image" tabindex="5"><img src="https://cdn.mywpku.com/20190629095631.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nicechord - 现代和弦代号]]></title>
        <id>https://blog.g4.cx/post/nicechord-001</id>
        <link href="https://blog.g4.cx/post/nicechord-001">
        </link>
        <updated>2019-06-26T13:00:34.000Z</updated>
        <summary type="html"><![CDATA[<p>和弦的定义变得越来越宽泛，比较基本的解释就是：<strong>三度音或三度音以上的堆叠</strong>。</p>
]]></summary>
        <content type="html"><![CDATA[<p>和弦的定义变得越来越宽泛，比较基本的解释就是：<strong>三度音或三度音以上的堆叠</strong>。</p>
<!-- more -->
<h2 id="和弦命名">和弦命名</h2>
<p><strong>三和弦 triad</strong>：三个音的和弦</p>
<p>「三和弦」之后命名规则发生变化——以「最后一个和弦音的度数」来命名，也就是：</p>
<ul>
<li><strong>七和弦 seventh</strong>：四个音的和弦（ex. 1 3 5 7）</li>
<li><strong>九和弦 ninth</strong>：五个音的和弦（ex. 1 3 5 7 9）</li>
<li><strong>十一和弦 ...</strong>：六个音的和弦（ex. 1 3 5 7 9 11）</li>
<li><strong>十三和弦 ...</strong>：七个音的和弦（ex. 1 3 5 7 9 11 13）</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://cdn.mywpku.com/20190626210847.png" alt="十三和弦就是尽头了" loading="lazy"></figure>
<p>十三和弦就是尽头了，因为第 15 度的音实际上与根音相同（也就是常说的高两个八度）。</p>
<h2 id="和弦家族和代号">和弦家族和代号</h2>
<p><strong>大（major）</strong>：</p>
<figure data-type="image" tabindex="2"><img src="https://cdn.mywpku.com/20190626211701.png" alt="Cmaj 系列" loading="lazy"></figure>
<p>仅有大三和弦不加字尾。大调总给人一种明亮、愉悦的感受，所能听到的欢快的曲目基本上都选择了大调音阶。</p>
<p><strong>小（minor）</strong>：</p>
<figure data-type="image" tabindex="3"><img src="https://cdn.mywpku.com/20190626211808.png" alt="Cm 系列" loading="lazy"></figure>
<p>小三和弦也是哦。和大调相反，小调呈现出的是灰色、哀伤等负面情绪。</p>
<p><strong>属（dominant）</strong>：</p>
<figure data-type="image" tabindex="4"><img src="https://cdn.mywpku.com/20190626211841.png" alt="C属和弦系列" loading="lazy"></figure>
<p>属和弦独特的效果在蓝调音乐中起到重要的作用。</p>
<p>之间的转换也有规律：例如大 7️⃣和弦转变到小7️⃣和弦，则需要给第 3️⃣、第 7️⃣度音降半音；转变到属 7️⃣和弦则需要给第 7️⃣度音降半音。</p>
<figure data-type="image" tabindex="5"><img src="https://cdn.mywpku.com/20190626211100.png" alt="大小属之间" loading="lazy"></figure>
<p>那么之后的音怎么办呢？不管和弦堆叠了几个音，其中的第 9️⃣、第十一、第十三度音都指「大调音阶」上的音。</p>
<h2 id="和弦变换">和弦变换</h2>
<p><strong>挂（sus）</strong>：</p>
<p>不要 3️⃣度音，换成 4️⃣或 2️⃣度音（默认为 4️⃣）：</p>
<figure data-type="image" tabindex="6"><img src="https://cdn.mywpku.com/20190626212213.png" alt="" loading="lazy"></figure>
<p><strong>加（add）</strong>：</p>
<figure data-type="image" tabindex="7"><img src="https://cdn.mywpku.com/20190626212340.png" alt="" loading="lazy"></figure>
<p>多弹一个音，在省略记法里偶数代表 add，而奇数就是 dominant 啦。</p>
<p><strong>不弹（omit）</strong>：</p>
<figure data-type="image" tabindex="8"><img src="https://cdn.mywpku.com/20190626212240.png" alt="" loading="lazy"></figure>
<p><strong>转位和弦</strong>：</p>
<p>通常情况下和弦的根音都应该是「最低音」，比如说 C 的大三和弦 <strong>C E G</strong> 啦。那么反过来——最低音不是根音的和弦就叫「转位和弦」。例如<strong>E G c</strong>：这时 <strong>C</strong> 被抬高了 8️⃣度，<strong>E</strong> 虽然是最低音但根音还是<strong>c</strong>，这也就是 <strong>C/E（C on E）</strong> 和弦。</p>
<blockquote>
<p>根音就是转位和弦的「原身」——某一个原位<s>原味</s>和弦建构的基础，它位于原位和弦的最下面（最低音），通过在上方堆叠音的方式来构成和弦。</p>
</blockquote>
<p>转位和弦又叫分数和弦，分子为根音 / 分母为最低音。</p>
<figure data-type="image" tabindex="9"><img src="https://cdn.mywpku.com/20190626221929.png" alt="" loading="lazy"></figure>
<p>Mark 一下这篇好文章：<a href="https://zhuanlan.zhihu.com/p/22211196">https://zhuanlan.zhihu.com/p/22211196</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ES6 - Set 和 Map 数据结构]]></title>
        <id>https://blog.g4.cx/post/es6-set-he-map-shu-ju-jie-gou</id>
        <link href="https://blog.g4.cx/post/es6-set-he-map-shu-ju-jie-gou">
        </link>
        <updated>2019-06-09T11:04:20.000Z</updated>
        <content type="html"><![CDATA[<h2 id="set-集合">Set 集合</h2>
<p>ES6 当中的 Set 和数学概念中的「集合」基本上是一致的。只不过它兼有普通 Array 的性质并且具备以下特点：</p>
<ul>
<li><strong>唯一</strong>：保证集合中各个元素都是不相同的</li>
<li><strong>不发生类型转换</strong>：<code>5</code> 和 <code>&quot;5&quot;</code> 被视为两种不同元素，<code>0</code> 和 <code>false</code> 也是如此</li>
<li><strong>NaN 等于自身</strong>：类似于精确相等运算符（<code>===</code>），主要的区别是 <code>NaN</code> 等于自身，而精确相等运算符认为 <code>NaN</code> 不等于自身。（Via <a href="http://es6.ruanyifeng.com/#docs/set-map">Set 和 Map 数据结构</a>）</li>
<li><strong>空对象不相等</strong>：所有的空对象在集合中都被视为不同的元素</li>
</ul>
<p>其余的用法基本上和 Array 相同，利用 Set 集合很容易实现数学概念当中的「交并补集」，例如：</p>
<pre><code class="language-javascript">// 并集
let a = new Set([2, 3, 4, 5])
let b = new Set([4, 5, 7, 8])
let union = new Set([...a, ...b])
// [2, 3, 4, 5, 7, 8]

// 交集
let a = new Set([2, 3, 4, 5])
let b = new Set([4, 5, 7, 8])
let intersection = new Set([...a].filter(item =&gt; b.has(item)))
// [4, 5]

// 补集
let a = new Set([1, 2, 3, 4, 5, 6])
let b = new Set([1, 2, 3])
let c = new Set([...a].filter(item =&gt; !b.has(item)))
// [4, 5, 6]
</code></pre>
<p>还多学到一个知识点：剩余参数的内部实现是 <code>for..of</code>，所以具有 <code>Iterable</code> 接口的对象都能使用剩余参数 <code>...</code>。</p>
<p><code>new Set()</code> 当中可传递的不仅仅是数组，而是所有可进行遍历的对象（比如字符串）：</p>
<pre><code class="language-javascript">// 字母去重
let a = new Set(&quot;aaaaabbbcccccddd&quot;)
// Set(4) {&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;}
</code></pre>
<h2 id="map">Map</h2>
<p>在实现 <strong>「键-&gt;值」对</strong> 时我们通常使用的是 <code>Object</code>，但 <code>Object</code> 的设计更像是实现了 <strong>「字符串-&gt;值」对</strong> 的模式，这意味着键名必须是一个合法的字符串（或是 <code>Symbol</code>）。<code>Map</code> 的设计则更加开放，键值均可为任意类型。</p>
<p>和普通 <code>Object</code> 的用法稍有不同，<code>Map</code> 需要以方法 <code>set()</code> 和 <code>get()</code> 对实例进行操作，例如：</p>
<pre><code class="language-javascript">let a = new Map()
let b = { foo: 'bar' }
a.set(b, 'barbarbar')
// 我怎么知道这有什么用呢，反正我是把一个对象和 'barbarbar' 联系起来了
a.get(b)
// 得到 'barbarbar'
</code></pre>
<p>其实耐心尝试可以发现：<code>Map</code> 实际上是一个类似于 Hash 结构的存在，还是使用上面的例子，然后这样做会如何呢？</p>
<pre><code class="language-javascript">a.get({ foo: 'bar' }) // 提示不存在
</code></pre>
<p>这是因为 <strong>只有对同一个对象的引用，Map 结构才将其视为同一个键</strong> （Thanks ruanyifeng）。换句话来说，以下的操作会出现的对应结果是：</p>
<pre><code class="language-javascript">a.set('a', 'b')
a.get('a') // undefined

const keyA = 'a'
a.set(keyA, 'b')
a.get(keyA) // 'b'
</code></pre>
<p>那你可能会问了：<code>a.set('a', 'b')</code> 既然都没有报错，那我要怎么取到值啊？<br>
答：转换成其它形式呗（比如 <code>Array</code>, <code>Object</code>）~</p>
<h3 id="map-和-array-之间有些暧昧的关系">Map 和 Array 之间有些暧昧的关系：</h3>
<figure data-type="image" tabindex="1"><img src="https://i.loli.net/2018/10/16/5bc55f315c755.jpg" alt="" loading="lazy"></figure>
<p>From MDN：<a href="https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Map">Map 与数组的关系</a>，<code>Map</code> 的实现方法与构造二维数组有关。</p>
<pre><code class="language-javascript">var kvArray = [[&quot;key1&quot;, &quot;value1&quot;], [&quot;key2&quot;, &quot;value2&quot;]];

// 使用常规的Map构造函数可以将一个二维键值对数组转换成一个Map对象
var myMap = new Map(kvArray);

myMap.get(&quot;key1&quot;); // 返回值为 &quot;value1&quot;

// 使用Array.from函数可以将一个Map对象转换成一个二维键值对数组
console.log(Array.from(myMap)); // 输出和kvArray相同的数组

// 或者在键或者值的迭代器上使用Array.from，进而得到只含有键或者值的数组
console.log(Array.from(myMap.keys())); // 输出 [&quot;key1&quot;, &quot;key2&quot;]
</code></pre>
]]></content>
    </entry>
</feed>